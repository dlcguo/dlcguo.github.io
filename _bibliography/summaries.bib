
@article{2506.05348v2,
Author        = {Yifan Wang and Peishan Yang and Zhen Xu and Jiaming Sun and Zhanhua Zhang and Yong Chen and Hujun Bao and Sida Peng and Xiaowei Zhou},
Title         = {FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic
  Scene Reconstruction},
Eprint        = {2506.05348v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper addresses the challenge of reconstructing dynamic 3D scenes with
complex motions. Some recent works define 3D Gaussian primitives in the
canonical space and use deformation fields to map canonical primitives to
observation spaces, achieving real-time dynamic view synthesis. However, these
methods often struggle to handle scenes with complex motions due to the
difficulty of optimizing deformation fields. To overcome this problem, we
propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives
to appear at arbitrary time and locations. In contrast to canonical Gaussian
primitives, our representation possesses the strong flexibility, thus improving
the ability to model dynamic 3D scenes. In addition, we endow each Gaussian
primitive with an motion function, allowing it to move to neighboring regions
over time, which reduces the temporal redundancy. Experiments results on
several datasets show that the rendering quality of our method outperforms
recent methods by a large margin. Project page:
https://zju3dv.github.io/freetimegs/ .},
Year          = {2025},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2506.05348v2},
File          = {2506.05348v2.pdf},
EprintNoVer = {2506.05348}
}

@article{2312.12337v4,
Author        = {David Charatan and Sizhe Li and Andrea Tagliasacchi and Vincent Sitzmann},
Title         = {pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable
  Generalizable 3D Reconstruction},
Eprint        = {2312.12337v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.12337v4},
File          = {2312.12337v4.pdf},
EprintNoVer = {2312.12337}
}
